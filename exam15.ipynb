{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1-لابه ای برای مدل بنویسید که متن زیر را گرفته علامت های نگارشی را حذف کرده و سایز کلمات را تا 5 در نظر بگیردو خروجی  به صورت عددی باشد\n",
        "\n",
        "text = \"are/ you ready .?! yes iam resdy&()\"\n"
      ],
      "metadata": {
        "id": "TPSTTQn4hn7A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "x5F-46RyWQel"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import one_hot\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "ZNVxt4ntYnSh"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"are/ you ready .?! yes iam resdy&()\"\n",
        "text = re.sub(r'[^\\w\\s]','',text) \n",
        "text = text.lower()\n",
        "# tokens = word_tokenize(text) \n",
        "# tokens = [token for token in tokens if len(token) <= 5] "
      ],
      "metadata": {
        "id": "e2ytBGM2WcuR"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text)"
      ],
      "metadata": {
        "id": "FNDFGqp26aVQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(text)"
      ],
      "metadata": {
        "id": "o641RZtp8ikD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 5\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "id": "xnxfqEVw8mMn"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "names_ints_pad = sequence.pad_sequences(names_ints, maxlen=max_len)"
      ],
      "metadata": {
        "id": "3o-GXd5LOpqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(padded_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxIO4wMc8ttH",
        "outputId": "b4a518d6-5734-456e-fa53-732b944b4a7d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 3  0  0  0  0]\n",
            " [ 4  0  0  0  0]\n",
            " [ 1  0  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 2  0  0  0  0]\n",
            " [ 7  0  0  0  0]\n",
            " [ 8  0  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 4  0  0  0  0]\n",
            " [ 1  0  0  0  0]\n",
            " [ 3  0  0  0  0]\n",
            " [ 5  0  0  0  0]\n",
            " [ 2  0  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 2  0  0  0  0]\n",
            " [ 1  0  0  0  0]\n",
            " [ 6  0  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 9  0  0  0  0]\n",
            " [ 3  0  0  0  0]\n",
            " [10  0  0  0  0]\n",
            " [ 0  0  0  0  0]\n",
            " [ 4  0  0  0  0]\n",
            " [ 1  0  0  0  0]\n",
            " [ 6  0  0  0  0]\n",
            " [ 5  0  0  0  0]\n",
            " [ 2  0  0  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "unique_words = list(set(tokens))\n",
        "اگر متن طولانی با کلمات تکراری داشتیم کلمات را یونیک میکردیم"
      ],
      "metadata": {
        "id": "n30lVZO7YD7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(text) \n",
        "tokens = [token for token in tokens if len(token) <= 5] "
      ],
      "metadata": {
        "id": "oIDsOkRGOGSP"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_words = [one_hot(word, len(tokens)) for word in tokens]\n"
      ],
      "metadata": {
        "id": "7rRxZvc3YPjS"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_9Q8qMnYi06",
        "outputId": "38b72219-51ea-4917-f943-fbb19abdc62e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3], [3], [2], [5], [5], [4]]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max([len(encoded_word) for encoded_word in encoded_words])\n",
        "padded_texts = pad_sequences(encoded_words, maxlen=max_length, padding='post')\n"
      ],
      "metadata": {
        "id": "OVHk91JmkWL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(set(tokens)) + 1\n",
        "embedding_dim = 100"
      ],
      "metadata": {
        "id": "G5TSC7v4mKLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
        "model.add(LSTM(units=64))\n",
        "model.add(Dense(units=1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "bcGux7NrjtOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "LFjPoKfsmNae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(padded_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAjlJuAyoJiX",
        "outputId": "1c103773-cce5-45c3-d002-44501c0081c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 355ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jg-s0no4ZiJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "spXwztzcYrti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2-با استفاده از \n",
        "\n",
        "tokenizer \n",
        "\n",
        "جمله زیر را توکن بندی کنید و ایندکس های تخصیص داده شده را نشان دهید\n",
        "\n",
        "test_data = [\n",
        "  \"Enjoy coffee this morning.\",\n",
        "  \"I enjoy going to the supermarket.\",\n",
        "  \"Want some milk for your coffee?\"\n",
        "]"
      ],
      "metadata": {
        "id": "uogtIMERlzcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = [ \"Enjoy coffee this morning.\", \"I enjoy going to the supermarket.\", \"Want some milk for your coffee?\" ]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(test_data)\n"
      ],
      "metadata": {
        "id": "p-hdsalLF3JL"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences = tokenizer.texts_to_sequences(test_data)"
      ],
      "metadata": {
        "id": "4dxQKmUEF8Fk"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bb0HWCHXH7O3",
        "outputId": "0fd49e5b-a4ae-448c-c931-1c0cb8a9041f"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'enjoy': 1, 'coffee': 2, 'this': 3, 'morning': 4, 'i': 5, 'going': 6, 'to': 7, 'the': 8, 'supermarket': 9, 'want': 10, 'some': 11, 'milk': 12, 'for': 13, 'your': 14}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence, tokenized_sentence in zip(test_data, tokenized_sentences):\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(f\"Token IDs: {tokenized_sentence}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep_REkOnGAP7",
        "outputId": "c8245b40-ec22-4ba2-bab8-7a2b4247e9f6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: Enjoy coffee this morning.\n",
            "Token IDs: [1, 2, 3, 4]\n",
            "\n",
            "Sentence: I enjoy going to the supermarket.\n",
            "Token IDs: [5, 1, 6, 7, 8, 9]\n",
            "\n",
            "Sentence: Want some milk for your coffee?\n",
            "Token IDs: [10, 11, 12, 13, 14, 2]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3-دو روش برخورد با داده های متنی به طور کلی چیست و چه زمانی از کدام روش استقاده میکنیم؟\n"
      ],
      "metadata": {
        "id": "lJlnrCOEnJT5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Bag of Words (BoW)\n",
        " \n",
        " The bag-of-words model is a simplifying representation used in natural language processing and information retrieval (IR). In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity. The bag-of-words model has also been used for computer vision\n",
        "The bag-of-words model is commonly used in methods of document classification where the (frequency of) occurrence of each word is used as a feature for training a classifier.\n",
        "The Bag-of-words model is one example of a Vector space model.\n",
        "\n",
        " Word Embedding\n",
        "\n",
        "In natural language processing (NLP), a word embedding is a representation of a word. The embedding is used in text analysis. Typically, the representation is a real-valued vector that encodes the meaning of the word in such a way that words that are closer in the vector space are expected to be similar in meaning.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZUAMJrEuMyCv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LTIBGqmfGEaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "linux\n",
        "\n",
        "1- فرمانی بنویسید که متنی را خوانده و هر جا اسم شما بود را با کلمه دیگری \n",
        "جایگزین کرده و آن را در فایل جدیدی ذخیره کند\n",
        "\n",
        "2- پرینت کنید services ردیف سوم فایل\n",
        "\n",
        "3- vimدستورات زیر در \n",
        "\n",
        "چکاری انجام می دهند\n",
        "\n",
        "/\n",
        "\n",
        "escape\n",
        "\n",
        ":\n",
        "\n",
        ":wq\n",
        "\n",
        "x\n",
        "\n",
        ":q!\n",
        "\n",
        "u\n",
        "\n",
        "ctl + r\n",
        "\n",
        "d+4+d\n",
        "\n",
        "p\n",
        "\n",
        "y+2+y\n",
        "\n",
        "dd\n",
        "\n",
        "yy\n",
        "\n",
        "\n",
        "4- کامندی برای کنترل رمان استفاده از لینوکس\n"
      ],
      "metadata": {
        "id": "Aol8RBXOoSJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sed 's/fateme/chico/g' input.txt > output.txt\n"
      ],
      "metadata": {
        "id": "T1uImSMMDv9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sed  3p file.txt\n"
      ],
      "metadata": {
        "id": "lS61Cr-KEr-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ": دستورات و کامندها شوع میشوند\n"
      ],
      "metadata": {
        "id": "wnPW1YY4HUM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "x برای حذف کاراکتر "
      ],
      "metadata": {
        "id": "CxNLlsNhG9ME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ":wq با اجرای این دستور تغییرات ذخیره و سپس خارج میشود"
      ],
      "metadata": {
        "id": "sg3xvFd4EyhP"
      }
    }
  ]
}