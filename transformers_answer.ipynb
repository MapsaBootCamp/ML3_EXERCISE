{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### جواب های خود را به فارسی و زبان خود بنویسید"
      ],
      "metadata": {
        "id": "G9xxwb9WQqsO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "۱- برای چه کاری استفاده میشود و ایراد آن چیست word embeding از تکنیک"
      ],
      "metadata": {
        "id": "rpwz9jmdDZ19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "برای تبدیل کلمات به بردارهای عددی استفاده میشود که برای تولید این بردارهای عددی از الگوریتم های متفاوتی میتوانیم استفاده کنیم مانند Word2Vec، GloVe و FastText \n",
        "\n",
        "از مشکلات این تکنیک میتوان به موارد زیر اشاره کرد\n",
        "\n",
        "عدم در نظر گرفتن ترتیب کلمات\n",
        "\n",
        "نیاز به داده ای آموزشی زیادی دارد تا روابط معنایی کلمات را به خوبی نشان دهد \n",
        "\n",
        "نمیتواند برای کلماتی که چند معنی دارند خوب عمل کند \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GCMyUwt8GRRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "۲- چیست self-attention منظور از "
      ],
      "metadata": {
        "id": "QTBRXKLVMmKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "در شبکه های عصبی  ترنسفورمر استفاده میشود که هر توکن در یک دنباله یا جمله به تمام توکن های دیگر در دنباله توجه میکند که این باعث میشود بردار ویژگی خو در را با توجه به ارتباطات داخل دنباله تعیین کند در واقع هر توکن یک بردار ویژگی میگیرد که شامل یک ترکیب از ویژگی های تمام توکن های موجود در دنباله یا جمله است برای محاسبه بردار ویژگی هر توکن ابتدا به همه توکن‌های دیگر در دنباله وزن‌هایی نسبت داده می‌شود که نشان دهنده اهمیت هر توکن در تولید بردار ویژگی هستند سپس با استفاده از این وزن‌ها یک ترکیب خطی از بردارهای ویژگی تمام توکن‌ها محاسبه می‌شود که به عنوان بردار ویژگی نهایی برای هر توکن استفاده می‌شود در واقع در این تکنیک توجه به مهم تر در جمله بیشتر است و ویژگی هایی که بیشترین تاثیر را در جمله دارند به خوبی استخراج میشوند    "
      ],
      "metadata": {
        "id": "DTZT_6zNOkl4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "۳- منظور از اصطلاح زیر چیست\n",
        "\n",
        "multi head attention\n",
        "\n",
        "position embeding"
      ],
      "metadata": {
        "id": "cKKpBs4DP1qZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "multi head attention\n",
        "\n",
        "از چندین  self-attention کنار هم استفاده شده و خروجی هریک از آنها با هو ترکیب  میشود تا خروجی نهایی بدست آید که اینکار باعث میشود درک بهتری از اطلاعات ورودی داشته باشیم   "
      ],
      "metadata": {
        "id": "gxC-MD0dU-Q1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "position embeding\n",
        "\n",
        "چون اطلاعات مربوط به ترتیب کلمات نیز به ورودی داده میشود از این مفهوم استفاده میکنیم \n",
        "در واقع یک ماتریس که شامل اطلاعات پوزیشن کلمات و امبدینگ بردار کلمات است برای محاسبه از توابع سینوسی و کسینوسی استفاده میکنیم\n",
        "\n"
      ],
      "metadata": {
        "id": "1kEa9rKURt1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "۴- دو بخش اصلی ترنسفورمر ها چیست\n",
        "دو مدل زبر چگونه بوجود آمدند\n",
        "\n",
        "Bert gpt"
      ],
      "metadata": {
        "id": "8pwpnOJJRxTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder\n",
        "Decoder\n",
        "\n"
      ],
      "metadata": {
        "id": "EscoEyxqYV1c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bert\n",
        "\n",
        "Bidirectional Encoder Representations from Transformers \n",
        "توسط تیم پژوهشی شرکت گوگل در سال ۲۰۱۸ معرفی شد\n",
        "از ساختار ترنسفورمر استفاده میکند که فقط از لایه‌های انکدر استفاده می‌کند و لایه دیکدر را ندارد برای پردازش یک جمله، از همه کلمات قبل و بعد از آن کلمه استفاده می‌کند و در نتیجه مدل، یک بردار حالت برای هر کلمه تولید می‌کند که حاوی اطلاعات دو جهت جمله است(Bidirectional)"
      ],
      "metadata": {
        "id": "kOiOGs0BYcnA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT\n",
        "\n",
        "د  یک مدل زبانی ایجاد می کند که با دریافت یک جمله شروع، قادر به پیش بینی کلمات بعدی در جمله است برای این کار مدل به صورت خودکار به طور تدریجی شروع به یادگیری روابط بین کلمات در متن می کند و به دنبال یافتن الگوهایی است که بهترین نتیجه را در پیش بینی کلمات بعدی می دهد\n",
        "فقط از لایه دیکدر استفاده میکند"
      ],
      "metadata": {
        "id": "X79HTs49fF65"
      }
    }
  ]
}