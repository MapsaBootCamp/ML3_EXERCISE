{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "261f5a90-e58e-4f6c-a1c8-c808d06a01bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing\n",
    "import matplotlib.pyplot as plt # image plotting\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c66ea5-edae-4bc2-b56f-4f397f8e3c1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### KaggleCode to import Pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b4f8980-099b-461e-a06a-63f5126a269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ca0c64d-650b-47d8-a417-3a2d7abacd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = 'images'\n",
    "annotations_path = \"annotations\"\n",
    "images = [*os.listdir(\"images\")]\n",
    "output_data_path =  '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d42a2b3a-4bb2-4760-a97d-339b4638e8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET # is used to parse an XML (inherently hierarchical) data format, which is the format of the annotations file\n",
    "\n",
    "def parse_annotation(path):\n",
    "    tree = ET.parse(path)\n",
    "    root = tree.getroot()\n",
    "    constants = {}\n",
    "    objects = [child for child in root if child.tag == 'object']\n",
    "    for element in tree.iter():\n",
    "        if element.tag == 'filename':\n",
    "            constants['file'] = element.text[0:-4]\n",
    "        if element.tag == 'size':\n",
    "            for dim in list(element):\n",
    "                if dim.tag == 'width':\n",
    "                    constants['width'] = int(dim.text)\n",
    "                if dim.tag == 'height':\n",
    "                    constants['height'] = int(dim.text)\n",
    "                if dim.tag == 'depth':\n",
    "                    constants['depth'] = int(dim.text)\n",
    "    object_params = [parse_annotation_object(obj) for obj in objects]\n",
    "    #print(constants)\n",
    "    full_result = [merge(constants,ob) for ob in object_params]\n",
    "    return full_result   \n",
    "\n",
    "\n",
    "def parse_annotation_object(annotation_object):\n",
    "    params = {}\n",
    "    for param in list(annotation_object):\n",
    "        if param.tag == 'name':\n",
    "            params['name'] = param.text\n",
    "        if param.tag == 'bndbox':\n",
    "            for coord in list(param):\n",
    "                if coord.tag == 'xmin':\n",
    "                    params['xmin'] = int(coord.text)              \n",
    "                if coord.tag == 'ymin':\n",
    "                    params['ymin'] = int(coord.text)\n",
    "                if coord.tag == 'xmax':\n",
    "                    params['xmax'] = int(coord.text)\n",
    "                if coord.tag == 'ymax':\n",
    "                    params['ymax'] = int(coord.text)\n",
    "            \n",
    "    return params       \n",
    " \n",
    "def merge(dict1, dict2):\n",
    "    res = {**dict1, **dict2}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e10ebf26-9439-4ed5-9e47-ec59eecb8a84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4072, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "dataset = [parse_annotation(anno) for anno in glob.glob(annotations_path+\"/*.xml\") ]\n",
    "\n",
    "# Since the output of the parse_annotation function is a list of lists, we need to flatten the ctopped faces.\n",
    "# i.e make it a list of images instead of a list of lists.\n",
    "full_dataset = sum(dataset, []) # \n",
    "#full_dataset\n",
    "\n",
    "df = pd.DataFrame(full_dataset)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "596e2cfd-9aa0-462a-a163-505e6a0a183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_image = 'maksssksksss0' # chose the image\n",
    "df_final_test = df.loc[df[\"file\"] == final_test_image] # create a separate dataframe which contain only the people in this specific image\n",
    "images.remove(f'{final_test_image}.png') # remove the image from the full dataset\n",
    "df = df.loc[df[\"file\"] != final_test_image] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2082b2a9-6af2-4501-9fc5-1f67fd4acb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'file':'file_name', 'name':'label'}, inplace = True)\n",
    "df_final_test.rename(columns = {'file':'file_name', 'name':'label'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ee32b64-6c7a-4d8b-ba49-ed42c70323ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "with_mask                3231\n",
       "without_mask              715\n",
       "mask_weared_incorrect     123\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"label\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ac9d13f-94ed-4003-93cd-bca588bab75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['label'].unique()\n",
    "directory = ['train', 'test', 'val']\n",
    "output_data_path =  '.'\n",
    "\n",
    "import os\n",
    "for label in labels:\n",
    "    for d in directory:\n",
    "        path = os.path.join(output_data_path, d, label)\n",
    "        #print(path)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afc3f15f-eca5-4c74-be18-84f613c77b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def crop_img(image_path, x_min, y_min, x_max, y_max):\n",
    "    \n",
    "    '''\n",
    "     This function takes an image path + x and y coordinates of two opposite corners of the rectangle \n",
    "     and returns a cropped image\n",
    "    '''\n",
    "    x_shift = (x_max - x_min) * 0.1\n",
    "    y_shift = (y_max - y_min) * 0.1\n",
    "    img = Image.open(image_path)\n",
    "    cropped = img.crop((x_min - x_shift, y_min - y_shift, x_max + x_shift, y_max + y_shift))\n",
    "    return cropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23b89543-1580-442f-922e-151fbe6b4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_faces(image_name, image_info):\n",
    "    \n",
    "    '''\n",
    "     This function takes an image name + dataframe with information about the image \n",
    "     and splits the image into all the different faces. image name contains the \n",
    "     upper-left coordinate of each face so we could distinguish it later\n",
    "    '''\n",
    "    faces = []\n",
    "    df_one_img = image_info[image_info['file_name'] == image_name[:-4]][['xmin', 'ymin', 'xmax', 'ymax', 'label']]\n",
    "    #print(df_one_img)\n",
    "    for row_num in range(len(df_one_img)):\n",
    "        x_min, y_min, x_max, y_max, label = df_one_img.iloc[row_num] \n",
    "        image_path = os.path.join(input_data_path, image_name)\n",
    "        faces.append((crop_img(image_path, x_min, y_min, x_max, y_max), label,f'{image_name[:-4]}_{(x_min, y_min)}'))\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4ddc32e-48ca-4ac8-9074-90387ad78476",
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_faces = [extract_faces(img, df) for img in images]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d50bc529-0ddb-4547-8d65-7bc53300b4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_cropped_faces = sum(cropped_faces, [])\n",
    "with_mask = [(img, image_name) for img, label,image_name in flat_cropped_faces if label == \"with_mask\"]\n",
    "mask_weared_incorrect = [(img, image_name) for img, label,image_name in flat_cropped_faces if label == \"mask_weared_incorrect\"]\n",
    "without_mask = [(img, image_name) for img, label,image_name in flat_cropped_faces if label == \"without_mask\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c959af4d-d481-47d7-a211-47cbb45a1ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of images with mask: 3231\n",
      "num of images without mask: 715\n",
      "num of images incorrect mask: 123\n",
      "sum: 4069\n"
     ]
    }
   ],
   "source": [
    "print(f'num of images with mask: {len(with_mask)}')\n",
    "print(f'num of images without mask: {len(without_mask)}')\n",
    "print(f'num of images incorrect mask: {len(mask_weared_incorrect)}')\n",
    "print(f'sum: {len(with_mask) + len(without_mask) + len(mask_weared_incorrect) }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26365e12-10b8-4056-a300-e505f55e8b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_with_mask, test_with_mask = train_test_split(with_mask, test_size=0.20, random_state=42)\n",
    "test_with_mask, val_with_mask = train_test_split(test_with_mask, test_size=0.7, random_state=42)\n",
    "\n",
    "train_mask_weared_incorrect, test_mask_weared_incorrect = train_test_split(mask_weared_incorrect, test_size=0.20, random_state=42)\n",
    "test_mask_weared_incorrect, val_mask_weared_incorrect = train_test_split(test_mask_weared_incorrect, test_size=0.7, random_state=42)\n",
    "\n",
    "train_without_mask, test_without_mask = train_test_split(without_mask, test_size=0.20, random_state=42)\n",
    "test_without_mask, val_without_mask = train_test_split(test_without_mask, test_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1fd3a611-048c-4451-b715-0dbe54b38c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image, image_name, output_data_path,  dataset_type, label):\n",
    "    '''\n",
    "     This function takes an image name + a path of output folder\n",
    "     and saves image into the output folder\n",
    "    '''\n",
    "\n",
    "    output_path = os.path.join(output_data_path, dataset_type, label ,f'{image_name}.png')\n",
    "    image.save(output_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5087e65d-303b-431f-b515-0b5f41ddb8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, image_name in train_with_mask:\n",
    "    save_image(image, image_name, output_data_path, 'train', 'with_mask')\n",
    "\n",
    "for image, image_name in train_mask_weared_incorrect:\n",
    "    save_image(image, image_name, output_data_path, 'train', 'mask_weared_incorrect')\n",
    "\n",
    "for image, image_name in train_without_mask:\n",
    "    save_image(image, image_name, output_data_path, 'train', 'without_mask')\n",
    "        \n",
    "# Test set\n",
    "\n",
    "for image, image_name in test_with_mask:\n",
    "    save_image(image, image_name, output_data_path, 'test', 'with_mask')\n",
    "\n",
    "for image, image_name in test_mask_weared_incorrect:\n",
    "    save_image(image, image_name, output_data_path, 'test', 'mask_weared_incorrect')\n",
    "\n",
    "for image, image_name in test_without_mask:\n",
    "    save_image(image, image_name, output_data_path, 'test', 'without_mask')\n",
    "    \n",
    "# Val set\n",
    "    \n",
    "for image, image_name in val_with_mask:\n",
    "    save_image(image, image_name, output_data_path, 'val', 'with_mask')\n",
    "\n",
    "for image, image_name in val_without_mask:\n",
    "    save_image(image, image_name, output_data_path, 'val', 'without_mask')\n",
    "\n",
    "for image, image_name in val_mask_weared_incorrect:\n",
    "    save_image(image, image_name, output_data_path, 'val', 'mask_weared_incorrect')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ca5ea77b-6d52-4de9-b22e-6b955de3c23f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3254 images belonging to 3 classes.\n",
      "Found 572 images belonging to 3 classes.\n",
      "Found 247 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255, horizontal_flip=True, zoom_range=0.1, shear_range=0.2, width_shift_range=0.1,\n",
    "    height_shift_range=0.1, rotation_range=4, vertical_flip=False\n",
    "\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(  rescale=1.0 / 255)\n",
    " \n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    directory='train', \n",
    "    target_size = (35,35),\n",
    "    class_mode=\"categorical\", batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Validation data\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    directory='val', \n",
    "    target_size = (35,35),\n",
    "    class_mode=\"categorical\", batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "# Test data\n",
    "test_generator = val_datagen.flow_from_directory(\n",
    "    directory='test', \n",
    "    target_size = (35,35),\n",
    "    class_mode=\"categorical\", batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bd79fc19-b8fe-48dc-96e2-2d87fa16425a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps_per_epoch: 50\n",
      "val_steps: 9\n"
     ]
    }
   ],
   "source": [
    "data_size = len(train_generator) \n",
    "#data_size2 = train_generator.n\n",
    "\n",
    "#print(f\"data_size: {data_size}, {data_size2}\")\n",
    "\n",
    "steps_per_epoch = int(data_size / batch_size)\n",
    "print(f\"steps_per_epoch: {steps_per_epoch}\")\n",
    "\n",
    "val_steps = int(len(val_generator) // batch_size)\n",
    "#print(f\"val size: {len(val_generator)}\")\n",
    "print(f\"val_steps: {val_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4808e01f-8433-41b3-b5db-f28502c3672d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Kaggle Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a481b708-7246-4485-84a1-34e95e58a838",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 14:56:34.348888: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-28 14:56:34.348995: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-28 14:56:58.780837: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 14:56:58.781219: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 14:56:58.781239: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2023-02-28 14:57:11.481591: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-28 14:57:11.493260: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-28 14:57:11.493468: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (e20175251072): /proc/driver/nvidia/version does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 35, 35, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 17, 17, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 17, 17, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 4, 4, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               512500    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 1503      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 537,587\n",
      "Trainable params: 537,587\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 16, kernel_size = 3,  padding='same', activation = 'relu', input_shape = (35,35,3)))\n",
    "model.add(MaxPooling2D(pool_size = 2))\n",
    "model.add(Conv2D(filters = 32, kernel_size = 3,  padding='same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = 2))\n",
    "model.add(Conv2D(filters = 64, kernel_size = 3,  padding='same', activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = 2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 500, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units = 3, activation = 'softmax'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ca2dbd9d-ed39-4736-98d2-7213406247b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy', 'Recall', 'Precision', 'AUC'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a8c7bd3-8418-4252-b110-ad82b463b9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afd42f75-5668-4f0a-a361-542ec3e20e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "lrr = ReduceLROnPlateau(monitor='val_loss',patience=8,verbose=1,factor=0.5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb4365-41f9-424a-8eef-942f225d6b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on train data\n",
    "\n",
    "model_history = model.fit_generator(\n",
    "    generator=train_generator,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    shuffle=True,\n",
    "    #verbose=2,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=val_steps,\n",
    "    callbacks=[early_stopping, lrr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f38e668e-d42c-40e5-a9ec-438cc6dae317",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40065e4d-4fd0-47a0-b718-dca8cbaddc3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### My Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fee511b6-849d-4910-a83a-4373b4ec068b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ade80ce-5cf4-4b89-a299-cde05e5f0a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def loadimages(path,w=20,h=20):\n",
    "    files= glob.glob(path)\n",
    "    imarray=[]\n",
    "    for imagefile in files[1:15]:\n",
    "        image = tf.keras.utils.load_img(imagefile, grayscale=True ,target_size=(w,h))\n",
    "        input_arr=tf.keras.utils.img_to_array(image)\n",
    "        imarray.append(input_arr.reshape(w,h))\n",
    "    imarray = np.array(imarray)\n",
    "    return(imarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "81568664-f98b-4773-8f0b-b7f8cc0d923c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2, 2)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9b59214c-e7a7-4bf7-8f92-07894b28b873",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_testWM=loadimages('./test/without_mask/*.*')\n",
    "X_testM=loadimages('./test/with_mask/*.*')\n",
    "X_trainM=loadimages('./train/with_mask/*.*')\n",
    "X_trainWM=loadimages('./train/without_mask/*.*')\n",
    "X_valM=loadimages('./val/with_mask/*.*')\n",
    "X_valWM=loadimages('./val/without_mask/*.*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "a343ff46-fbd3-49bf-b678-c5bec0373f7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 20, 20)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_trainM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "e6e54388-5817-40d0-a3c6-7616c25b8d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2605 images belonging to 3 classes.\n",
      "Found 113 images belonging to 3 classes.\n",
      "Found 247 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "image_generator = ImageDataGenerator(\n",
    "        rescale=1/255,\n",
    "        rotation_range=10, # rotation\n",
    "        width_shift_range=0.2, # horizontal shift\n",
    "        height_shift_range=0.2, # vertical shift\n",
    "        zoom_range=0.2, # zoom\n",
    "        horizontal_flip=True, # horizontal flip\n",
    "        brightness_range=[0.2,1.2],# brightness\n",
    "        validation_split=0.2,)\n",
    "\n",
    "#Train & Validation Split\n",
    "train_dataset = image_generator.flow_from_directory(batch_size=32,\n",
    "                                                 directory='train',\n",
    "                                                 shuffle=True,\n",
    "                                                 target_size=(20, 20),\n",
    "                                                 subset=\"training\",\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "validation_dataset = image_generator.flow_from_directory(batch_size=32,\n",
    "                                                 directory='val',\n",
    "                                                 shuffle=True,\n",
    "                                                 target_size=(20, 20),\n",
    "                                                 subset=\"validation\",\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "test_dataset = image_generator.flow_from_directory(batch_size=32,\n",
    "                                                 directory='test',\n",
    "                                                 shuffle=True,\n",
    "                                                 target_size=(20, 20),\n",
    "                                                 # subset=\"testing\",\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "\n",
    "#Organize data for our predictions\n",
    "image_generator_submission = ImageDataGenerator(rescale=1/255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80445908-1f5a-4bc9-b29c-20de8651e79f",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3068fd95-1527-412e-b110-9b0a50659fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56078434, 0.34117648, 0.26666668], dtype=float32)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0][0][0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4c59ca9f-db6e-4db9-bce5-ea782dc30ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape = [20, 20,3]),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "    keras.layers.Conv2D(64, (2, 2), activation='relu'),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "    keras.layers.Conv2D(64, (2, 2), activation='relu'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(100, activation='relu'),\n",
    "    keras.layers.Dense(3, activation ='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e75df083-2a8c-43f4-9969-1a8d663f67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "             loss = 'binary_crossentropy',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8bcc0e71-a70c-4282-b44c-174d3ffab86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "82/82 [==============================] - 55s 631ms/step - loss: 0.3396 - accuracy: 0.7954 - val_loss: 0.2771 - val_accuracy: 0.8142\n",
      "Epoch 2/20\n",
      "82/82 [==============================] - 6s 68ms/step - loss: 0.2381 - accuracy: 0.8729 - val_loss: 0.1870 - val_accuracy: 0.9204\n",
      "Epoch 3/20\n",
      "82/82 [==============================] - 6s 67ms/step - loss: 0.1982 - accuracy: 0.8971 - val_loss: 0.2046 - val_accuracy: 0.8673\n",
      "Epoch 4/20\n",
      "82/82 [==============================] - 6s 68ms/step - loss: 0.1882 - accuracy: 0.8994 - val_loss: 0.1812 - val_accuracy: 0.8938\n",
      "Epoch 5/20\n",
      "82/82 [==============================] - 6s 68ms/step - loss: 0.1867 - accuracy: 0.9060 - val_loss: 0.1786 - val_accuracy: 0.9027\n",
      "Epoch 6/20\n",
      "82/82 [==============================] - 6s 67ms/step - loss: 0.1814 - accuracy: 0.9010 - val_loss: 0.1747 - val_accuracy: 0.9204\n",
      "Epoch 7/20\n",
      "82/82 [==============================] - 6s 68ms/step - loss: 0.1681 - accuracy: 0.9202 - val_loss: 0.1639 - val_accuracy: 0.9115\n",
      "Epoch 8/20\n",
      "82/82 [==============================] - 6s 68ms/step - loss: 0.1786 - accuracy: 0.9144 - val_loss: 0.1256 - val_accuracy: 0.9292\n",
      "Epoch 9/20\n",
      "82/82 [==============================] - 6s 67ms/step - loss: 0.1732 - accuracy: 0.9075 - val_loss: 0.1661 - val_accuracy: 0.8938\n",
      "Epoch 10/20\n",
      "82/82 [==============================] - 6s 69ms/step - loss: 0.1675 - accuracy: 0.9159 - val_loss: 0.1742 - val_accuracy: 0.9469\n",
      "Epoch 11/20\n",
      "82/82 [==============================] - 6s 67ms/step - loss: 0.1652 - accuracy: 0.9159 - val_loss: 0.1484 - val_accuracy: 0.9292\n",
      "Epoch 12/20\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 0.1506 - accuracy: 0.9205 - val_loss: 0.1174 - val_accuracy: 0.9115\n",
      "Epoch 13/20\n",
      "82/82 [==============================] - 6s 69ms/step - loss: 0.1517 - accuracy: 0.9259 - val_loss: 0.1425 - val_accuracy: 0.9292\n",
      "Epoch 14/20\n",
      "82/82 [==============================] - 6s 68ms/step - loss: 0.1594 - accuracy: 0.9190 - val_loss: 0.1296 - val_accuracy: 0.9381\n",
      "Epoch 15/20\n",
      "82/82 [==============================] - 6s 70ms/step - loss: 0.1484 - accuracy: 0.9232 - val_loss: 0.1542 - val_accuracy: 0.9292\n",
      "Epoch 16/20\n",
      "82/82 [==============================] - 6s 69ms/step - loss: 0.1434 - accuracy: 0.9267 - val_loss: 0.1212 - val_accuracy: 0.9469\n",
      "Epoch 17/20\n",
      "82/82 [==============================] - 6s 69ms/step - loss: 0.1531 - accuracy: 0.9213 - val_loss: 0.1653 - val_accuracy: 0.9115\n",
      "Epoch 18/20\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 0.1480 - accuracy: 0.9259 - val_loss: 0.1578 - val_accuracy: 0.9204\n",
      "Epoch 19/20\n",
      "82/82 [==============================] - 6s 68ms/step - loss: 0.1474 - accuracy: 0.9263 - val_loss: 0.1322 - val_accuracy: 0.9292\n",
      "Epoch 20/20\n",
      "82/82 [==============================] - 6s 71ms/step - loss: 0.1470 - accuracy: 0.9267 - val_loss: 0.1154 - val_accuracy: 0.9292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f14836c82e0>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 8\n",
    "epochs = 50\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "model.fit(train_dataset, epochs=20, validation_data=validation_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "4b9cac16-094c-49a0-9376-825d983e42aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 86ms/step - loss: 0.1419 - accuracy: 0.9231\n",
      "Loss:  0.14189675450325012\n",
      "Accuracy:  0.9230769276618958\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print(\"Loss: \", loss)\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8466cb-8c4e-4866-823b-ade8289354e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848a804-7025-4789-83ef-ad1cc5a7d4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bdc7e0-275b-4179-bdf3-c51357fe9781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
