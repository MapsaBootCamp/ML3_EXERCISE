{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### جواب های خود را به فارسی و زبان خود بنویسید\n"
      ],
      "metadata": {
        "id": "tQD0RVNDoB2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.   برای چه کاری استفاده میشود و ایراد آن چیست word embeding از تکنیک\n",
        "\n",
        "---\n",
        "\n",
        "چون شبکه هاب عصبی متن رو متوجه نمیشن و باید اونها رو به بردار تبدیل کنیم. برای این کار روشهایی وجود داره مثل :\n",
        "\n",
        "word embeding و One-hot word vectors \n",
        "\n",
        "هر دوی این روشها برای تبدیل نوشته به بردار هست. .\n",
        "\n",
        "One-hot word vectors ماتریس وکتوری که میسازه اسپارس هست و دایمیشن اون زیاده و هارد کد هست اما \n",
        "\n",
        "Word embeddings دنس هست  و دایمیشن اون نسبت به قبلی کمتر است و بردار کلمات هارد کد نیستند بلکه شبکه خودش در طول اموزش یاد میگیره که چه وکتوری به کلمات بده\n",
        "\n",
        "ایراد اول اون پدینگ هست که با مسک کردن مشکل حل میشه \n",
        "\n",
        "اما ایراد دوم اینه که هر کلمه یه بردار داره و اون بردار به متن نگاه نمیکنه و و استاتیک هست و  با توجه به متن بردار تشکیل نمیشه \n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c1QoXMcloB6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.   چیست self-attention منظور از**\n",
        "\n"
      ],
      "metadata": {
        "id": "gxq9oZJYoB9D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "دو روشی که در بالا گفتم روشهایی برای تشکیل بردار برای کلمات هست.   \n",
        "سلف اتنشن هم همین کار رو میکنه و برای کلمات بردار فیچر میسازه . منتها برای ساختن بردار کلمات کاملا کانتکس ایبل است . یعنی به متن توجه میکند\n",
        "\n",
        "برای این کار کی در کا ضرب میشه\n",
        "اسکیل میشه \n",
        "از یه سافت مکس عبور میکنه\n",
        "خروجی در وی ضرب میشه\n",
        "یه سامیشن گرفته میشه\n",
        "\n",
        "عموما کا و وی یکسان هستند و کیو متفاوت هست\n",
        "مثلا برای ترجمه کا و وی زبان مبدا و کیو زبان مقصد است\n",
        "اما توی کلاسیفیکیشن هر سه یکسان هستند\n",
        "\n",
        "به لحاظ ریاضی هم توضیحش میشه\n",
        "outputs = sum(values * pairwise_scores(query, keys))"
      ],
      "metadata": {
        "id": "ki0zlct9oCAR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3.    منظور از اصطلاح زیر چیست**\n",
        "\n",
        "# *multi head attention  and position embeding *"
      ],
      "metadata": {
        "id": "xxlsM1umoCCl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **multi head attention**\n",
        "\n",
        "این کا و کیو و وی رو برای اینکه روشون عملیات دیپ اتفاق بیافته از یه شبکه دنس عبور باید داد تا فیچرهای مهم اون \n",
        "استخراج بشه\n",
        "\n",
        "اما ممکنه این شبکه های دنس همه فیچرها رو در نیاورده باشه\n",
        "\n",
        "برای همین \n",
        "\n",
        "چندین شبکه دنس رو بصورت مستقل قرار میدن تا مطمن بشن همه فیچر های لازم مدل استخراج شده و بعد خروجی اونها کانکت میشن\n",
        "\n",
        "به این سازوکار میگن مولتی هد لیر\n",
        "\n"
      ],
      "metadata": {
        "id": "HWhYhumQoCFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **position embeding**\n",
        "\n",
        "input embedding\n",
        "میاد \n",
        "vector\n",
        "کلمات توکن شده که عملیات های پری پراسس بر روی اون انجام شده رو میگیره و به هر کلمه یه بردار ثابت اختصاص میده. که جيوگرافیک رپرزنتیشن کلمات به سمنتیک اونها ارتباط داره. یعنی هر چه وکتور کلمات به هم نزدیک تر باشه معنی بیشتری با هم دارند\n",
        "اما میدانیم در تکست انچه مهم است \n",
        "order\n",
        "کلمات است . در مقاله اصلی ترنسفورمرها میادبرای تزریق جایگاه کلمات پوزیشن امبدینگ وارد میشه و میاد هر کلمه رو با \n",
        "پوزیشن اون جمع میکنه و فرمول پوزیشن اون هم بصورت زیر هست\n",
        "\n",
        "P E(pos,2i) = sin(pos/100002i/dmodel)\n",
        "\n",
        "P E(pos,2i+1) = cos(pos/100002i/dmodel)\n",
        "\n",
        "البته جناب شله هم برای حفظ پوزیشن میاد و یه لایه امبدینگ میزاره و فرق اون با روش بالا اینه که در روش شله پارامترها باید اپدیت بشه ولی در روش بالا احتیاجی به اپدیت پارامترها نیست"
      ],
      "metadata": {
        "id": "87cP3Gs1oCI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4.   بخش اصلی ترنسفورمر ها چیست و دو مدل زیر چگونه بوجود آمدند**\n",
        "\n",
        "Bert gpt"
      ],
      "metadata": {
        "id": "XDayP9rooCLa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ترنسفورمرها شبکه های عصبی برمبنای اتنشن هستن که دو قسمت اصلی دارند\n",
        "\n",
        "encoder and decoder"
      ],
      "metadata": {
        "id": "ucDJirYQoCOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **encoder:**\n",
        "\n",
        "انکودر ها ورودی رو میگیرن و به وکتور تبدیل میکنند. روی اونها پوزیشن انکودینگ انجام میشه که همان طور که در بالا گفته شد میتونه از روشهای مختلفی انجام بشه. بعد کیو و وی و کا به مولتی اتنشن ها وارد میشه که نتیجه اون با کا و کیو و وی جمع زده میشه و نرمالایزمیشه بعد این نتیجه به \n",
        "\n",
        "feed forward\n",
        "\n",
        "داده میشه . که این فیدفوروارد هم چند تا لایه دنس هست. نتیجه این مرحله هم با ورودی قبلش جمع میشه و نرمالایز میشه.\n",
        "\n",
        "از کل این فرایندهایی که در بالا توضیح داده شد چند تا رو میارن کنار هم و بهش میگن انکودر که در مقاله اصلی ۶ تا بودن\n",
        "\n",
        "\n",
        "خروجی اینها فیچرهای مورد نیازی بودن که دیتکت شدن ما میتونیم این قسمت رو بزاریک کنار و در هر جای دیگه ای که میخواهیم استفاده کنیم. برت دقیقا همین استکه روی ۲ تا تسک مهم \n",
        "\n",
        "mask language model and next sequence prediction\n",
        "\n",
        "بصورت همزمان اموزش داده شده است. الان ما میتونیم این مدل پری ترین شده برت رو برداریم و برای مسپله خودمون فاینن تیون \n",
        "کنیم\n",
        "\n",
        "Bidirectional Encoder Representations from Transformers"
      ],
      "metadata": {
        "id": "aGD9ylrwoCRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **decoder:**\n",
        "\n",
        "خیلی شبیه انکودر هست فقط \n",
        "\n",
        "mask multi head attention \n",
        "\n",
        "هم دارد. در حقیقت این قسمت \n",
        "\n",
        "casual mask\n",
        "\n",
        "هست. یعنی هر کلمه رو فقط نسبت به کلمات قبل نگاه میکنن در حالی که در مولتی هد اتنشن هر کلمه نسبت به همه کلمات دیده میشه.\n",
        "جی پی تی ها دیکودر بیس هستند\n",
        "\n",
        "Generative Pre-trained Transformer"
      ],
      "metadata": {
        "id": "Gmy8nAxSoCVN"
      }
    }
  ]
}